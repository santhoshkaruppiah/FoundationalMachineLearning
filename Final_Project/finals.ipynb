{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data and understand it. The dataset contains 2 continous feature variables and 1 target categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('train_set.txt',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns = ['x1','x2','target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build new attributes\n",
    "\n",
    "Define new attributes as per definition below.\n",
    "\n",
    "X 3 = X1^2 \n",
    "X4 = X2 \n",
    "X5 = X1X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['x3'] = test_df.x1*test_df.x1\n",
    "test_df['x4'] = test_df.x2*test_df.x2\n",
    "test_df['x5'] = test_df.x1*test_df.x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>target</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.775408</td>\n",
       "      <td>23.986692</td>\n",
       "      <td>r</td>\n",
       "      <td>0.601257</td>\n",
       "      <td>575.361405</td>\n",
       "      <td>18.599466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.170503</td>\n",
       "      <td>-3.287474</td>\n",
       "      <td>r</td>\n",
       "      <td>850.918251</td>\n",
       "      <td>10.807487</td>\n",
       "      <td>-95.897279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.739044</td>\n",
       "      <td>-28.033329</td>\n",
       "      <td>r</td>\n",
       "      <td>45.414707</td>\n",
       "      <td>785.867535</td>\n",
       "      <td>-188.917824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.216100</td>\n",
       "      <td>22.013695</td>\n",
       "      <td>r</td>\n",
       "      <td>10.343297</td>\n",
       "      <td>484.602776</td>\n",
       "      <td>70.798239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.374906</td>\n",
       "      <td>7.925541</td>\n",
       "      <td>g</td>\n",
       "      <td>2244.381691</td>\n",
       "      <td>62.814197</td>\n",
       "      <td>375.471748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x1         x2 target           x3          x4          x5\n",
       "0   0.775408  23.986692      r     0.601257  575.361405   18.599466\n",
       "1  29.170503  -3.287474      r   850.918251   10.807487  -95.897279\n",
       "2   6.739044 -28.033329      r    45.414707  785.867535 -188.917824\n",
       "3   3.216100  22.013695      r    10.343297  484.602776   70.798239\n",
       "4  47.374906   7.925541      g  2244.381691   62.814197  375.471748"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_df_X = test_df[['x1','x2']]\n",
    "simple_df_y = test_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target categorical variable is converted to nummerical values using one-hot encoding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "enc = OneHotEncoder()\n",
    "Y = enc.fit_transform(simple_df_y[:, np.newaxis]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets scale data using StandardScaler with mean 0 and variance 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(simple_df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is now split into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled, Y, test_size=0.5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = simple_df_X.shape[1]\n",
    "n_classes = Y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets resuse method with minor modifications from TA lecture. \n",
    "The modfifcations is to take different number of neurons and activation for each layer on a given network model.\n",
    "\n",
    "The final output layer is softmax since this is classification task. We could use as well sigmoid linear activation function for binary classification task. For this task, since we do one-hot encoding, we see output values are 0 or 1. So, I choose sigmoid function on final layer vs softmax. This increased the accuracy.\n",
    "\n",
    "Some of different choice of activations functions tried below are:\n",
    "\n",
    "sigmoid - Output goes between 0 to 1. This mostly used on the final layer instead of hidden layers. Its not good idea to combine hidden layers with linear activations and final layer with linear activations. Which means its simple logistic regression without learning much\n",
    "\n",
    "tanh - Output goes between -1 to 1.\n",
    "\n",
    "Relu - Output goes between 0 to infinity.\n",
    "\n",
    "Leaky Relu - Output goes between (-infinity to infinity)\n",
    "\n",
    "Based on observations, few factors improve the model like number of neuron going high, choice of activations and layers. Its evident from below with respect to activations that sigmoid is poor choice on hidden layers and good for final layer when we have binary classification. \n",
    "\n",
    "The RELU activations functions on hidden layers (layers between input and output) performs better than 'tanh'. With regards to number of neuron the accucarcy increases number of neurons to learn. As of number of neurons increases accuracy increase along with runtime. So, its trade off to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def create_model_network(input_dim, output_dim, neurons, activations, layers=1, name='model'):\n",
    "    def create_model():\n",
    "        # Create model\n",
    "        model = Sequential(name=name)\n",
    "        for i in range(layers):\n",
    "            model.add(Dense(neurons[i], input_dim=input_dim, activation=activations[i]))\n",
    "        model.add(Dense(output_dim, activation='sigmoid'))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer='adam', \n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "    return create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below method is enhanced version from our TA lecture nb. This method takes care of reporting metrics for given models along with train, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "history_dict = {}\n",
    "\n",
    "# TensorBoard Callback\n",
    "cb = TensorBoard()\n",
    "\n",
    "def measure(models,X_train,Y_train,X_test,Y_test):\n",
    "    for create_model in models:\n",
    "        model = create_model()\n",
    "        print('Model name:', model.name)\n",
    "        history_callback = model.fit(X_train, Y_train,\n",
    "                                     batch_size=5,\n",
    "                                     epochs=50,\n",
    "                                     verbose=0,\n",
    "                                     validation_data=(X_test, Y_test),\n",
    "                                     callbacks=[cb])\n",
    "        score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        history_dict[model.name] = [history_callback, model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the simplest neural networks\n",
    "\n",
    "Use above method to create multiple models with varying activation, neurons and layer.\n",
    "Then, for each model print summary.\n",
    "\n",
    "Create upto 4 models with each model having 'i+1' layers from 1 to 4. The activations and neuron for each layer is passed along as argument.\n",
    "\n",
    "Now, first lets try relu activation models with varied layers and neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_109 (Dense)            (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 51\n",
      "Trainable params: 51\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_111 (Dense)            (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 371\n",
      "Trainable params: 371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_114 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 2,307\n",
      "Trainable params: 2,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "activations_list = [\n",
    "    ['relu','relu','relu','relu'],\n",
    "    ['relu','relu','relu','relu'],\n",
    "    ['relu','relu','relu','relu'],\n",
    "    ['relu','relu','relu','relu']\n",
    "]\n",
    "neurons_list = [\n",
    "    [8,8,8,8],\n",
    "    [16,16,16,16],\n",
    "    [32,32,32,32],\n",
    "    [64,64,64,64]\n",
    "               ]\n",
    "\n",
    "model_relu = [create_model_network(n_features, n_classes, neurons_list[i-1], activations_list[i-1], i, 'model_relu_{}-layer'.format(i)) \n",
    "          for i in range(1, 4)]\n",
    "\n",
    "for create_model in model_relu:\n",
    "    create_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: model_relu_1-layer\n",
      "Test loss: 0.20454975152224825\n",
      "Test accuracy: 0.9005847960187677\n",
      "Model name: model_relu_2-layer\n",
      "Test loss: 0.06124815869836779\n",
      "Test accuracy: 0.9385964901823747\n",
      "Model name: model_relu_3-layer\n",
      "Test loss: 0.05101616887582673\n",
      "Test accuracy: 0.9502923966151232\n"
     ]
    }
   ],
   "source": [
    "measure(model_relu,X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets try tanh activation models with varied layers and neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_127 (Dense)            (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 51\n",
      "Trainable params: 51\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_129 (Dense)            (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 371\n",
      "Trainable params: 371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_132 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 2,307\n",
      "Trainable params: 2,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "activations_list = [\n",
    "    ['tanh','tanh','tanh','tanh'],\n",
    "    ['tanh','tanh','tanh','tanh'],\n",
    "    ['tanh','tanh','tanh','tanh'],\n",
    "    ['tanh','tanh','tanh','tanh']\n",
    "]\n",
    "neurons_list = [\n",
    "    [8,8,8,8],\n",
    "   [16,16,16,16],\n",
    "    [32,32,32,32],\n",
    "    [64,64,64,64]\n",
    "               ]\n",
    "\n",
    "model_tanh = [create_model_network(n_features, n_classes, neurons_list[i-1], activations_list[i-1], i, 'model_tanh_{}-layer'.format(i)) \n",
    "          for i in range(1, 4)]\n",
    "\n",
    "for create_model in model_tanh:\n",
    "    create_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: model_tanh_1-layer\n",
      "Test loss: 0.4029647450000919\n",
      "Test accuracy: 0.8801169597614579\n",
      "Model name: model_tanh_2-layer\n",
      "Test loss: 0.23033862086067422\n",
      "Test accuracy: 0.8976608204562762\n",
      "Model name: model_tanh_3-layer\n",
      "Test loss: 0.10796705358906795\n",
      "Test accuracy: 0.8918128672399019\n"
     ]
    }
   ],
   "source": [
    "measure(model_tanh,X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets try sigmoid activation models with varied layers and neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_91 (Dense)             (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 51\n",
      "Trainable params: 51\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_93 (Dense)             (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 371\n",
      "Trainable params: 371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_96 (Dense)             (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 2,307\n",
      "Trainable params: 2,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "activations_list = [\n",
    "    ['sigmoid','sigmoid','sigmoid','sigmoid'],\n",
    "    ['sigmoid','sigmoid','sigmoid','sigmoid'],\n",
    "    ['sigmoid','sigmoid','sigmoid','sigmoid'],\n",
    "    ['sigmoid','sigmoid','sigmoid','sigmoid']\n",
    "]\n",
    "neurons_list = [\n",
    "    [8,8,8,8],\n",
    "    [16,16,16,16],\n",
    "    [32,32,32,32],\n",
    "    [64,64,64,64]               \n",
    "]\n",
    "model_sigmoid = [create_model_network(n_features, n_classes, neurons_list[i-1], activations_list[i-1], i, 'model_sigmoid_{}-layer'.format(i)) \n",
    "          for i in range(1, 4)]\n",
    "\n",
    "for create_model in model_sigmoid:\n",
    "    create_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: model_sigmoid_1-layer\n",
      "Test loss: 0.46810212975357013\n",
      "Test accuracy: 0.8684210533287093\n",
      "Model name: model_sigmoid_2-layer\n",
      "Test loss: 0.46840840578079224\n",
      "Test accuracy: 0.8684210533287093\n",
      "Model name: model_sigmoid_3-layer\n",
      "Test loss: 0.42832385447987337\n",
      "Test accuracy: 0.8888888906317147\n"
     ]
    }
   ],
   "source": [
    "measure(model_sigmoid,X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already defined new features from definitions below. We added 3 new columns like x3,x4,x5.\n",
    "\n",
    "X3 = X1^2 \n",
    "\n",
    "X4 = X2 \n",
    "\n",
    "X5 = X1X2\n",
    "\n",
    "Below we will use different subset of data from combinations of x1...x5 features and create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_df_X_3_4 = test_df[['x3','x4']]\n",
    "simple_df_X_3_5 = test_df[['x3','x5']]\n",
    "simple_df_X_3_4_5 = test_df[['x3','x4','x5']]\n",
    "simple_df_X_1_2_3_4_5 = test_df[['x1','x2','x3','x4','x5']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create models for features with x3, x4.\n",
    "\n",
    "We dont have any change on the target variables which is using one-hot encoding values. As discussed previously, we will go with 'sigmoid' for final layer. For hidden layers, we will choose activations functions like relu, tanh and will test with different neurons count. \n",
    "\n",
    "Its evident from below, that tanh activation function performs better than relu with high accuracy comparitvely. its consistent irrespective of number of neurons it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled_3_4 = scaler.fit_transform(simple_df_X_3_4)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train_3_4, X_test_3_4, Y_train, Y_test = train_test_split(\n",
    "    X_scaled_3_4, Y, test_size=0.5, random_state=2)\n",
    "\n",
    "n_features = simple_df_X_3_4.shape[1]\n",
    "n_classes = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_332 (Dense)            (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "dense_333 (Dense)            (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 51\n",
      "Trainable params: 51\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_334 (Dense)            (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "dense_335 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_336 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 371\n",
      "Trainable params: 371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_337 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dense_338 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_339 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_340 (Dense)            (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 2,307\n",
      "Trainable params: 2,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model name: model_relu_1-layer\n",
      "Test loss: 0.024696758049621917\n",
      "Test accuracy: 1.0\n",
      "Model name: model_relu_2-layer\n",
      "Test loss: 0.045739731767721344\n",
      "Test accuracy: 0.9795321626969945\n",
      "Model name: model_tanh_3-layer\n",
      "Test loss: 0.05435411409850706\n",
      "Test accuracy: 1.0\n",
      "Model name: model_tanh_4-layer\n",
      "Test loss: 0.002058581619629669\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "activations_list = [\n",
    "    ['relu','relu','relu','relu'],\n",
    "    ['relu','relu','relu','relu'],\n",
    "    ['tanh','tanh','tanh','tanh'],\n",
    "    ['tanh','tanh','tanh','tanh']\n",
    "]\n",
    "neurons_list = [\n",
    "    [8,8,8,8],\n",
    "    [64,64,64,64],\n",
    "    [8,8,8,8],\n",
    "    [64,64,64,64]\n",
    "               ]\n",
    "model_3_4 = [create_model_network(n_features, n_classes, neurons_list[i-1], activations_list[i-1], i, 'model_{}_{}-layer'.format(activations_list[i-1][0],i)) \n",
    "          for i in range(1, 5)]\n",
    "\n",
    "for create_model in model_relu:\n",
    "    create_model().summary()\n",
    "\n",
    "measure(model_3_4,X_train_3_4,Y_train,X_test_3_4,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create models for features with x3, x5.\n",
    "\n",
    "We dont have any change on the target variables which is using one-hot encoding values. As discussed previously, we will go with 'sigmoid' for final layer. For hidden layers, we will choose activations functions like relu, tanh and will test with different neurons count. \n",
    "\n",
    "It evident from below results that activation function with relu performs better than tanh functions. As, neuron counts increase so does the accuracy increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled_3_5 = scaler.fit_transform(simple_df_X_3_5)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train_3_5, X_test_3_5, Y_train, Y_test = train_test_split(\n",
    "    X_scaled_3_5, Y, test_size=0.5, random_state=2)\n",
    "\n",
    "n_features = simple_df_X_3_5.shape[1]\n",
    "n_classes = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_355 (Dense)            (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "dense_356 (Dense)            (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 51\n",
      "Trainable params: 51\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_357 (Dense)            (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "dense_358 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_359 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 371\n",
      "Trainable params: 371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_360 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dense_361 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_362 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_363 (Dense)            (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 2,307\n",
      "Trainable params: 2,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model name: model_relu_1-layer\n",
      "Test loss: 0.23762662734901696\n",
      "Test accuracy: 0.9005847960187677\n",
      "Model name: model_relu_2-layer\n",
      "Test loss: 0.12033147125216255\n",
      "Test accuracy: 0.9473684227954574\n",
      "Model name: model_tanh_3-layer\n",
      "Test loss: 0.25542385640897247\n",
      "Test accuracy: 0.9005847960187677\n",
      "Model name: model_tanh_4-layer\n",
      "Test loss: 0.15591303019495736\n",
      "Test accuracy: 0.903508772626955\n"
     ]
    }
   ],
   "source": [
    "activations_list = [\n",
    "    ['relu','relu','relu','relu'],\n",
    "    ['relu','relu','relu','relu'],\n",
    "    ['tanh','tanh','tanh','tanh'],\n",
    "    ['tanh','tanh','tanh','tanh']\n",
    "]\n",
    "neurons_list = [\n",
    "    [8,8,8,8],\n",
    "    [64,64,64,64],\n",
    "    [8,8,8,8],\n",
    "    [64,64,64,64]\n",
    "               ]\n",
    "\n",
    "model_3_5 = [create_model_network(n_features, n_classes, neurons_list[i-1], activations_list[i-1], i, 'model_{}_{}-layer'.format(activations_list[i-1][0],i))\n",
    "          for i in range(1, 5)]\n",
    "\n",
    "for create_model in model_relu:\n",
    "    create_model().summary()\n",
    "\n",
    "measure(model_3_5,X_train_3_5,Y_train,X_test_3_5,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create models for features with x3, x4, x5.\n",
    "\n",
    "We dont have any change on the target variables which is using one-hot encoding values. As discussed previously, we will go with 'sigmoid' for final layer. For hidden layers, we will choose activations functions like relu, tanh and will test with different neurons count. \n",
    "\n",
    "It evident from below results that activation function with relu performs better than tanh functions. As, neuron counts increase so does the accuracy increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled_3_4_5 = scaler.fit_transform(simple_df_X_3_4_5)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train_3_4_5, X_test_3_4_5, Y_train, Y_test = train_test_split(\n",
    "    X_scaled_3_4_5, Y, test_size=0.5, random_state=2)\n",
    "\n",
    "n_features = simple_df_X_3_4_5.shape[1]\n",
    "n_classes = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_401 (Dense)            (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "dense_402 (Dense)            (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 51\n",
      "Trainable params: 51\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_403 (Dense)            (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "dense_404 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_405 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 371\n",
      "Trainable params: 371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_406 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dense_407 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_408 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_409 (Dense)            (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 2,307\n",
      "Trainable params: 2,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model name: model_relu_1-layer\n",
      "Test loss: 0.06219717346088231\n",
      "Test accuracy: 0.967836256264246\n",
      "Model name: model_relu_2-layer\n",
      "Test loss: 0.0005575452867145813\n",
      "Test accuracy: 1.0\n",
      "Model name: model_tanh_3-layer\n",
      "Test loss: 0.01177485019774639\n",
      "Test accuracy: 1.0\n",
      "Model name: model_tanh_4-layer\n",
      "Test loss: 0.0494655179437141\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "activations_list = [\n",
    "    ['relu','relu','relu','relu'],\n",
    "    ['relu','relu','relu','relu'],\n",
    "    ['tanh','tanh','tanh','tanh'],\n",
    "    ['tanh','tanh','tanh','tanh']\n",
    "]\n",
    "neurons_list = [\n",
    "    [8,8,8,8],\n",
    "    [64,64,64,64],\n",
    "    [8,8,8,8],\n",
    "    [64,64,64,64]\n",
    "               ]\n",
    "model_3_4_5 = [create_model_network(n_features, n_classes, neurons_list[i-1], activations_list[i-1], i, 'model_{}_{}-layer'.format(activations_list[i-1][0],i)) \n",
    "          for i in range(1, 5)]\n",
    "\n",
    "for create_model in model_relu:\n",
    "    create_model().summary()\n",
    "\n",
    "measure(model_3_4_5,X_train_3_4_5,Y_train,X_test_3_4_5,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create models for features with x1, x2, x3, x4, x5.\n",
    "\n",
    "We dont have any change on the target variables which is using one-hot encoding values. As discussed previously, we will go with 'sigmoid' for final layer. For hidden layers, we will choose activations functions like relu, tanh and will test with different neurons count. \n",
    "\n",
    "It evident from below results that activation function with tanh performs better than relu functions. As, neuron counts increase so does the accuracy increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled_1_2_3_4_5 = scaler.fit_transform(simple_df_X_1_2_3_4_5)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train_1_2_3_4_5, X_test_1_2_3_4_5, Y_train, Y_test = train_test_split(\n",
    "    X_scaled_1_2_3_4_5, Y, test_size=0.5, random_state=2)\n",
    "\n",
    "n_features = simple_df_X_1_2_3_4_5.shape[1]\n",
    "n_classes = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_424 (Dense)            (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "dense_425 (Dense)            (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 51\n",
      "Trainable params: 51\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_426 (Dense)            (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "dense_427 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_428 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 371\n",
      "Trainable params: 371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_429 (Dense)            (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dense_430 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_431 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_432 (Dense)            (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 2,307\n",
      "Trainable params: 2,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model name: model_relu_1-layer\n",
      "Test loss: 0.08220766157957546\n",
      "Test accuracy: 0.9298245631463347\n",
      "Model name: model_relu_2-layer\n",
      "Test loss: 0.0013175904044502942\n",
      "Test accuracy: 1.0\n",
      "Model name: model_tanh_3-layer\n",
      "Test loss: 0.05315148525419291\n",
      "Test accuracy: 0.9298245631463347\n",
      "Model name: model_tanh_4-layer\n",
      "Test loss: 0.001998504407959854\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "activations_list = [\n",
    "    ['relu','relu','relu','relu'],\n",
    "    ['relu','relu','relu','relu'],\n",
    "    ['tanh','tanh','tanh','tanh'],\n",
    "    ['tanh','tanh','tanh','tanh']\n",
    "]\n",
    "neurons_list = [\n",
    "    [8,8,8,8],\n",
    "    [64,64,64,64],\n",
    "    [8,8,8,8],\n",
    "    [64,64,64,64]\n",
    "               ]\n",
    "\n",
    "model_1_2_3_4_5 = [create_model_network(n_features, n_classes, neurons_list[i-1], activations_list[i-1], i, 'model_{}_{}-layer'.format(activations_list[i-1][0],i)) \n",
    "          for i in range(1, 5)]\n",
    "\n",
    "for create_model in model_relu:\n",
    "    create_model().summary()\n",
    "\n",
    "measure(model_1_2_3_4_5,X_train_1_2_3_4_5,Y_train,X_test_1_2_3_4_5,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
